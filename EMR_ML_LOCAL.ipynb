{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we fetch the data from Mongo DB into an EMR cluster and develop several machine learning models for aacurately predicting the event type from the eeg signals. \n",
    "We compare the performance of models in terms of both area under the ROC curve (AUC) and time taken to classify the test data.\n",
    "The raw EEG signals have been pre-processed and stored in Mongo DB along with their corresponding event data. The pre-processing step is part of another notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:10.499550Z",
     "start_time": "2019-01-20T21:50:07.501609Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"myApp\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:13.726800Z",
     "start_time": "2019-01-20T21:50:13.094801Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import *\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    ".config(\"spark.executor.memory\", \"22g\")\\\n",
    ".config(\"spark.driver.memory\", \"10g\").config(\"spark.memory.offHeap.enabled\",True)\\\n",
    ".config(\"spark.memory.offHeap.size\", \"3g\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:15.351654Z",
     "start_time": "2019-01-20T21:50:15.339644Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import asc, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:16.636638Z",
     "start_time": "2019-01-20T21:50:16.009658Z"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"../all/features/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:18.074275Z",
     "start_time": "2019-01-20T21:50:18.058587Z"
    }
   },
   "outputs": [],
   "source": [
    "def toFloat(x):\n",
    "    l = []\n",
    "    for i in range(len(x)):\n",
    "        if i < 1:\n",
    "            l.append(str(x[i]))\n",
    "        else: l.append(float(x[i]))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:19.438169Z",
     "start_time": "2019-01-20T21:50:18.687009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25',\n",
       " '0,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000',\n",
       " '1,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000',\n",
       " '2,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00003,0.00006,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:24.905035Z",
     "start_time": "2019-01-20T21:50:24.898767Z"
    }
   },
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda x: x.split(\",\")).map(lambda x: toFloat(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:27.522693Z",
     "start_time": "2019-01-20T21:50:25.742734Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('id', StringType(), True),\n",
    "                   StructField('0', DoubleType(), True),\n",
    "                   StructField('1', DoubleType(), True),\n",
    "                   StructField('2', DoubleType(), True),\n",
    "                   StructField('3', DoubleType(), True),\n",
    "                   StructField('4', DoubleType(), True),\n",
    "                   StructField('5', DoubleType(), True),\n",
    "                   StructField('6', DoubleType(), True),\n",
    "                   StructField('7', DoubleType(), True),\n",
    "                   StructField('8', DoubleType(), True),\n",
    "                   StructField('9', DoubleType(), True),\n",
    "                   StructField('10', DoubleType(), True),\n",
    "                   StructField('11', DoubleType(), True),\n",
    "                   StructField('12', DoubleType(), True),\n",
    "                   StructField('13', DoubleType(), True),\n",
    "                   StructField('14', DoubleType(), True),\n",
    "                   StructField('15', DoubleType(), True),\n",
    "                   StructField('16', DoubleType(), True),\n",
    "                   StructField('17', DoubleType(), True),\n",
    "                   StructField('18', DoubleType(), True),\n",
    "                   StructField('19', DoubleType(), True),\n",
    "                   StructField('20', DoubleType(), True),\n",
    "                   StructField('21', DoubleType(), True),\n",
    "                   StructField('22', DoubleType(), True),\n",
    "                   StructField('23', DoubleType(), True),\n",
    "                   StructField('24', DoubleType(), True),\n",
    "                   StructField('25', DoubleType(), True)\n",
    "                    ])\n",
    "df = spark.createDataFrame(rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:28.746552Z",
     "start_time": "2019-01-20T21:50:28.431100Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.filter(\"id != 'id'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:28.953724Z",
     "start_time": "2019-01-20T21:50:28.930226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- 0: double (nullable = true)\n",
      " |-- 1: double (nullable = true)\n",
      " |-- 2: double (nullable = true)\n",
      " |-- 3: double (nullable = true)\n",
      " |-- 4: double (nullable = true)\n",
      " |-- 5: double (nullable = true)\n",
      " |-- 6: double (nullable = true)\n",
      " |-- 7: double (nullable = true)\n",
      " |-- 8: double (nullable = true)\n",
      " |-- 9: double (nullable = true)\n",
      " |-- 10: double (nullable = true)\n",
      " |-- 11: double (nullable = true)\n",
      " |-- 12: double (nullable = true)\n",
      " |-- 13: double (nullable = true)\n",
      " |-- 14: double (nullable = true)\n",
      " |-- 15: double (nullable = true)\n",
      " |-- 16: double (nullable = true)\n",
      " |-- 17: double (nullable = true)\n",
      " |-- 18: double (nullable = true)\n",
      " |-- 19: double (nullable = true)\n",
      " |-- 20: double (nullable = true)\n",
      " |-- 21: double (nullable = true)\n",
      " |-- 22: double (nullable = true)\n",
      " |-- 23: double (nullable = true)\n",
      " |-- 24: double (nullable = true)\n",
      " |-- 25: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:30.984848Z",
     "start_time": "2019-01-20T21:50:29.637440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+-------+-------+-------+-------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "| id|  0|  1|  2|  3|  4|  5|      6|      7|      8|      9| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19| 20| 21| 22| 23| 24| 25|\n",
      "+---+---+---+---+---+---+---+-------+-------+-------+-------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|  0|0.0|0.0|0.0|0.0|0.0|0.0|    0.0|    0.0|    0.0|    0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n",
      "|  1|0.0|0.0|0.0|0.0|0.0|0.0|    0.0|    0.0|    0.0|    0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n",
      "|  2|0.0|0.0|0.0|0.0|0.0|0.0|    0.0|    0.0| 3.0E-5| 6.0E-5|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n",
      "|  3|0.0|0.0|0.0|0.0|0.0|0.0| 3.0E-5|    0.0| 3.3E-4| 5.6E-4|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n",
      "|  4|0.0|0.0|0.0|0.0|0.0|0.0| 2.1E-4| 9.0E-5|0.00215|0.00302|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n",
      "|  5|0.0|0.0|0.0|0.0|0.0|0.0| 9.8E-4| 8.9E-4| 0.0092|0.01064|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n",
      "|  6|0.0|0.0|0.0|0.0|0.0|0.0|0.00352|0.00536|0.02872| 0.0274|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n",
      "|  7|0.0|0.0|0.0|0.0|0.0|0.0|0.01001|0.02211| 0.0696|0.05607|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n",
      "|  8|0.0|0.0|0.0|0.0|0.0|0.0|0.02315|0.06847|0.13655|0.09603|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n",
      "|  9|0.0|0.0|0.0|0.0|0.0|0.0|0.04433|0.16781|0.22273|0.13943|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n",
      "+---+---+---+---+---+---+---+-------+-------+-------+-------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:42.897048Z",
     "start_time": "2019-01-20T21:50:42.444819Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "target_cols = [(str(x)) for x in range(0,6)]\n",
    "feature_cols = [(str(x)) for x in range(6,26)]\n",
    "\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=feature_cols)\n",
    "\n",
    "lpoints = va.transform(df).select(\"features\", (df['0']).alias('HandStart'),\\\n",
    "                                  (df['1']).alias('FirstDigitTouch'),\\\n",
    "                                  (df['2']).alias('BothStartLoadPhase'),\\\n",
    "                                  (df['3']).alias('LiftOff'),\\\n",
    "                                  (df['4']).alias('Replace'),\\\n",
    "                                  (df['5']).alias('BothReleased'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:45.881655Z",
     "start_time": "2019-01-20T21:50:45.571886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+------------------+-------+-------+------------+\n",
      "|            features|HandStart|FirstDigitTouch|BothStartLoadPhase|LiftOff|Replace|BothReleased|\n",
      "+--------------------+---------+---------------+------------------+-------+-------+------------+\n",
      "|          (20,[],[])|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|          (20,[],[])|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[2,3],[3.0E-5...|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,2,3],[3.0E...|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[2....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[9....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "|(20,[0,1,2,3],[0....|      0.0|            0.0|               0.0|    0.0|    0.0|         0.0|\n",
      "+--------------------+---------+---------------+------------------+-------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:46.345867Z",
     "start_time": "2019-01-20T21:50:45.992020Z"
    }
   },
   "outputs": [],
   "source": [
    "splits = lpoints.randomSplit([0.8,0.2])\n",
    "eeg_train = splits[0].cache()\n",
    "eeg_valid = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:52.691319Z",
     "start_time": "2019-01-20T21:50:52.662842Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T00:58:32.615414Z",
     "start_time": "2019-01-20T00:18:32.091272Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area Under ROC HandStart : 0.5219793799831756\n",
      "area Under ROC FirstDigitTouch : 0.5631759281062706\n",
      "area Under ROC BothStartLoadPhase : 0.5610746282201716\n",
      "area Under ROC LiftOff : 0.5777764418900291\n",
      "area Under ROC Replace : 0.5993477665100534\n",
      "area Under ROC BothReleased : 0.5822078067124323\n",
      "Time taken for logistic regression 2400.504782676697s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#labels are events that we are trying to classify\n",
    "labels= ['HandStart','FirstDigitTouch', 'BothStartLoadPhase', 'LiftOff', 'Replace', 'BothReleased']\n",
    "#iterating over the events and fitting a logisitic regression model and train it for each event\n",
    "start= time.time()\n",
    "for label in labels:\n",
    "    lr = LogisticRegression(regParam=0.01, maxIter=100, fitIntercept=True, labelCol=label)\n",
    "    lrmodel = lr.fit(eeg_train.select('features',label))\n",
    "    validpredicts = lrmodel.transform(eeg_valid.select('features',label))\n",
    "    bceval = BinaryClassificationEvaluator(labelCol=label)\n",
    "    auc = bceval.evaluate(validpredicts)\n",
    "    duration= time.time()-start\n",
    "    print ('area Under ROC ' + label+ \" : \" + str(auc))\n",
    "print('Time taken for logistic regression '+ str(duration) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T00:59:25.158794Z",
     "start_time": "2019-01-20T00:59:25.147036Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T02:04:50.355681Z",
     "start_time": "2019-01-20T00:59:25.877530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area Under ROC HandStart : 0.8569108168696032\n",
      "area Under ROC FirstDigitTouch : 0.8236509395086055\n",
      "area Under ROC BothStartLoadPhase : 0.8305707939287095\n",
      "area Under ROC LiftOff : 0.8203077534031135\n",
      "area Under ROC Replace : 0.8064275157300104\n",
      "area Under ROC BothReleased : 0.8407604777963608\n",
      "Time taken for Random Forest 3924.458245038986s\n"
     ]
    }
   ],
   "source": [
    "#labels are events that we are trying to classify\n",
    "labels= ['HandStart','FirstDigitTouch', 'BothStartLoadPhase', 'LiftOff', 'Replace', 'BothReleased']\n",
    "start= time.time()\n",
    "#iterating over the events and fitting a Random forest model and train it for each event\n",
    "for label in labels:\n",
    "    rf = RandomForestClassifier(maxDepth=10, labelCol=label)\n",
    "    rfmodel = rf.fit(eeg_train.select('features',label))\n",
    "    validpredicts = rfmodel.transform(eeg_valid.select('features',label))\n",
    "    bceval = BinaryClassificationEvaluator(labelCol=label)\n",
    "    auc = bceval.evaluate(validpredicts)\n",
    "    duration= time.time()-start\n",
    "    print ('area Under ROC ' + label+ \" : \" + str(auc))\n",
    "print('Time taken for Random Forest '+ str(duration) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T02:08:05.651925Z",
     "start_time": "2019-01-20T02:08:05.635979Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T07:34:42.556641Z",
     "start_time": "2019-01-20T02:08:05.959824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC HandStart : 0.49729977713214596\n",
      "areaUnderROC FirstDigitTouch : 0.5660934681431121\n",
      "areaUnderROC BothStartLoadPhase : 0.49135874243233607\n",
      "areaUnderROC LiftOff : 0.5268992717459255\n",
      "areaUnderROC Replace : 0.5472355448846024\n",
      "areaUnderROC BothReleased : 0.5200579970168281\n",
      "Time taken for Linear SVC  19596.57626605034s\n"
     ]
    }
   ],
   "source": [
    "#labels are events that we are trying to classify\n",
    "labels= ['HandStart','FirstDigitTouch', 'BothStartLoadPhase', 'LiftOff', 'Replace', 'BothReleased']\n",
    "start= time.time()\n",
    "#iterating over the events and fitting a logisitic regression model and train it for each event\n",
    "for label in labels:\n",
    "    svc = LinearSVC(labelCol=label)\n",
    "    svcmodel = svc.fit(eeg_train.select('features',label))\n",
    "    validpredicts = svcmodel.transform(eeg_valid.select('features',label))\n",
    "    bceval = BinaryClassificationEvaluator(labelCol=label)\n",
    "    auc = bceval.evaluate(validpredicts)\n",
    "    duration= time.time()-start\n",
    "    print ('areaUnderROC ' + label+ \" : \" + str(auc))\n",
    "print('Time taken for Linear SVC  '+ str(duration) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:50:55.415255Z",
     "start_time": "2019-01-20T21:50:55.407289Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T02:56:46.884576Z",
     "start_time": "2019-01-20T21:50:55.888603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC HandStart : 0.8451880202290933\n",
      "areaUnderROC FirstDigitTouch : 0.8136384438900479\n",
      "areaUnderROC BothStartLoadPhase : 0.8025608893173575\n",
      "areaUnderROC LiftOff : 0.8046389658993593\n",
      "areaUnderROC Replace : 0.7749284826024881\n",
      "areaUnderROC BothReleased : 0.8241372970951558\n",
      "Time taken for Gradient Boosted Tree  18350.97699737549s\n"
     ]
    }
   ],
   "source": [
    "labels= ['HandStart','FirstDigitTouch', 'BothStartLoadPhase', 'LiftOff', 'Replace', 'BothReleased']\n",
    "start= time.time()\n",
    "for label in labels:\n",
    "    gbt = GBTClassifier(maxIter=10, maxDepth=10, labelCol=label)\n",
    "    gbtmodel = gbt.fit(eeg_train.select('features',label))\n",
    "    validpredicts = gbtmodel.transform(eeg_valid.select('features',label))\n",
    "    bceval = BinaryClassificationEvaluator(labelCol=label)\n",
    "    auc = bceval.evaluate(validpredicts)\n",
    "    duration= time.time()-start\n",
    "    print ('areaUnderROC ' + label+ \" : \" + str(auc))\n",
    "print('Time taken for Gradient Boosted Tree  '+ str(duration) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
